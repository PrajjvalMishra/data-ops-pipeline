# ðŸ› ï¸ Data Operations Specialist Assignment â€“ Simulated Pipeline

This project demonstrates a complete data operations pipeline using a modern data stack. Due to restricted sandbox access, the solution uses **realistic mock data** and simulates all required logic to demonstrate a production-ready pipeline.

---

## âœ… Project Overview

| Step                     | Tool Used              | Status   |
|--------------------------|------------------------|----------|
| Data Ingestion           | Airbyte â†’ BigQuery     | âœ… Mock Data via Faker |
| Data Transformation      | dbt Cloud + BigQuery   | âœ… All models built |
| Data Testing             | dbt Tests              | âœ… `not_null`, `unique`, `custom test` |
| CI/CD                    | dbt Cloud Job          | âœ… Configured |
| Reporting                | Looker Studio          | âœ… Dashboard built |
| Alerting                 | BigQuery simulation    | âœ… Slack logic simulated |
| Documentation            | README + Loom Video    | âœ… Included |

---

## âš™ï¸ Tools Used

- **Airbyte** â€“ Ingested `products`, `purchases`, `users` (faker connector)
- **BigQuery** â€“ Cloud data warehouse
- **dbt Cloud** â€“ SQL modeling, tests, orchestration
- **Looker Studio** â€“ Dashboard visualization
- **Slack Alerts** â€“ Simulated via SQL query

---

## ðŸ“‚ Pipeline Steps

### ðŸ”¸ Step 1: Data Ingestion with Airbyte
- Faker connector used to simulate Amazon data
- Three tables ingested: `products`, `purchases`, `users`
- Stored in BigQuery under dataset: `airbyte_raw`

### ðŸ”¸ Step 2: dbt Models in BigQuery

| Model             | Description                      |
|-------------------|----------------------------------|
| `fct_orders`      | Order revenue metrics per SKU    |
| `fct_ads`         | Simulated ad spend per SKU       |
| `fct_inventory`   | Stock availability per SKU       |
| `mrt_kpi_daily`   | Final KPI table (joins all above)|

Models use Jinja templating, `ref()`, and CTEs.

---

## âœ… Data Testing

Implemented in `schema.yml`:
- `not_null` tests on key fields (e.g., `sku`, `order_date`)
- `unique` test on `order_id` in `fct_orders`
- ðŸ” **Custom test** on `days_of_cover > 0`

Custom test SQL:
``sql
SELECT * FROM {{ ref('mrt_kpi_daily') }} WHERE days_of_cover <= 0
All tests are run and verified via dbt Cloud.

ðŸ” CI/CD in dbt Cloud
![DBT Build](https://github.com/PrajjvalMishra/data-ops-pipeline/actions/workflows/dbt.yml/badge.svg)
dbt Cloud Job created with steps:

dbt deps

dbt build

dbt test

Job can be scheduled or triggered manually on push

ðŸ”” Slack Alert Simulation (No Webhook)
As real Slack webhook was not available, this query simulates the daily 09:00 EST alert:

![Slack Query](https://github.com/user-attachments/assets/91ebcfbb-133c-4940-a5d9-61fbcdfea124)

ðŸŽ¯ This would be used to alert:

Yesterdayâ€™s revenue

TACoS %

SKUs with < 15 days of stock

ðŸ–¼ï¸ Screenshot attached in Loom walkthrough.

ðŸ“Š Looker Studio Dashboard
Final model: mrt_kpi_daily

Components:

ðŸ“ˆ Revenue vs Ad Spend (Bar)

ðŸ§® TACoS trend (Line)

ðŸ“¦ Days of Cover (Cards)

ðŸ—‚ï¸ Filter by SKU and Date

ðŸ”— https://drive.google.com/file/d/1ptfjfwGwgAVmWaLlxZ4PWJoYHIlbSUvz/view?usp=sharing

ðŸ“½ï¸ Loom Video Walkthrough

ðŸ”— https://www.loom.com/share/108725ae73054488a4ee8e29481a1fb2?sid=7508f99b-0db3-4b60-83b7-15438dcaff6c


Covers:

Airbyte â†’ BigQuery

dbt Cloud models + tests

Slack alert logic

Looker dashboard

ðŸ™‹â€â™‚ï¸ Author
Prajjval Mishra
ðŸ“§ prajjvalmishra18@gmail.com
ðŸ”— [LinkedIn](https://www.linkedin.com/in/prajjvalmishra/)

âš ï¸ Note on Access
Sandbox credentials (SP-API, Ads API, Slack webhook, and GCP project) were not provided. This solution uses realistic mock data and simulates alerting logic. The entire pipeline is fully production-ready and can connect to live data sources with minimal changes once credentials are shared.

### ðŸ” Stretch Goals Not Implemented (Why)

The following stretch items were intentionally not implemented due to constraints:

- **SP-API Retry Logic**: Not applicable as real API credentials were not shared.
- **Pytest Unit Tests**: dbt handles all SQL logic and testing; no Python transformation layer present.
- **Metaplane Monitoring**: Requires integration with live data; simulated alerting logic used instead.

> All components are ready to be extended with these features once live credentials are available.
